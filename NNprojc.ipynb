{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "                                  PROJECT_C4\n",
        "\n",
        "                               \n",
        "                                   TEAM-76\n",
        " \n",
        "\n",
        "                            KRISHNA KOTARU(bkotaru)  \n",
        "                            GANESH THANU(gthanu)\n",
        "                            SHAUNAK PATEL(shpate25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7jLvgXvrnj1m"
      },
      "outputs": [],
      "source": [
        "# This cell handles the merge function\n",
        "import pandas as pd\n",
        "import os\n",
        "def merge_files(filex,filey):\n",
        "    #Read files\n",
        "    x_df = pd.read_csv(filex)\n",
        "    y_df = pd.read_csv(filey)\n",
        "\n",
        "    #Extract column integer values\n",
        "    x_first = pd.DataFrame(x_df.columns).T\n",
        "    y_first = pd.DataFrame(y_df.columns).T\n",
        "\n",
        "    #Define column names\n",
        "    x_columns = [\"a1\", \"a2\", \"a3\", \"g1\", \"g2\", \"g3\"]\n",
        "    y_columns = [\"y\"]\n",
        "\n",
        "    #Assign column names\n",
        "    x_df.columns = x_columns\n",
        "    y_df.columns= y_columns\n",
        "    x_first.columns = x_columns\n",
        "    y_first.columns = y_columns\n",
        "\n",
        "    #Concatenate the dataframes with new column names\n",
        "    x_df = pd.concat([x_first, x_df],axis = 0).reset_index(drop=True)\n",
        "    y_df = pd.concat([y_first, y_df],axis = 0).reset_index(drop=True)\n",
        "\n",
        "    #Define the output dataframe structure\n",
        "    combined_df = pd.DataFrame(columns=[\"a1\", \"a2\", \"a3\", \"g1\", \"g2\", \"g3\", \"y\"])\n",
        "    y_ind = 0\n",
        "\n",
        "    #Loop through and merge the dataframes\n",
        "    for i in range(0,len(x_df)):\n",
        "        if i%4 == 0: \n",
        "            y = y_df.iloc[y_ind:y_ind+1].reset_index(drop=True)\n",
        "            y_ind += 1\n",
        "        x = x_df.iloc[i:i+1].reset_index(drop=True)\n",
        "        z = pd.concat([x,y],axis=1).reset_index(drop=True)\n",
        "        combined_df = combined_df.append(z, ignore_index = True)\n",
        "    combined_df = combined_df.astype(float)\n",
        "    return combined_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gJEBlkKlrd34"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NrfooMRVFv4z"
      },
      "outputs": [],
      "source": [
        "# This cell contains the code for the implementation of sliding window\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "from statistics import mode\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def split_sequence_slide(Xval,yval,n_steps):\n",
        "  X , y = list(), list()\n",
        "  one_hot_dict = {3 : [0,0,0,1], 2 : [0,0,1,0], 1 : [0,1,0,0], 0 : [1,0,0,0]}\n",
        "  count = 0\n",
        "  while(count <= len(Xval)-n_steps):\n",
        "    arr_of_X_60 = np.array(Xval[count: count + n_steps,:], dtype=np.float32)\n",
        "    arr_of_y_60 = np.array(yval.iloc[count: count + n_steps, :])\n",
        "    count = count + 1\n",
        "\n",
        "    class_labels = {3:0,2:0,1:0,0:0}\n",
        "    for arr_y in arr_of_y_60:\n",
        "      if len(arr_y) > 0:\n",
        "        val = list(arr_y).index(1)\n",
        "        class_labels[val]+=1\n",
        "        \n",
        "    mode = max(class_labels, key=class_labels.get)\n",
        "    y.append(np.array(one_hot_dict[mode]))\n",
        "    X.append(arr_of_X_60)\n",
        "    \n",
        "    \n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73H00DGskyPG",
        "outputId": "7392326d-3b8a-4eb2-df3a-abd49ebf2775"
      },
      "outputs": [],
      "source": [
        "# This cell involves extracting values from the files, normalizing/encoding them  \n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  \n",
        "dict= {1:8, 2:5, 3:3, 4:2, 5:3 ,6:3, 7:4, 8:1 }\n",
        "one_hot_dict = {3 : [0,0,0,1], 2 : [0,0,1,0], 1 : [0,1,0,0], 0 : [1,0,0,0]}\n",
        "\n",
        "\n",
        "model_input_x,model_input_y = [],[]\n",
        "for i in dict.keys():\n",
        "    for j in range(1,dict[i]+1):\n",
        "        XInput = \"TrainingData/subject_00\"+str(i)+\"_0\"+str(j)+\"__x.csv\"\n",
        "        YInput = \"TrainingData/subject_00\"+str(i)+\"_0\"+str(j)+\"__y.csv\"\n",
        "\n",
        "        output_df = merge_files(XInput,YInput)\n",
        "\n",
        "        X,y = output_df[['a1','a2','a3','g1','g2','g3']],output_df['y']\n",
        "        y = y.fillna(0)\n",
        "        y = y.astype(int)\n",
        "        y_new = []\n",
        "        # Onehot encoding the output values\n",
        "        for class_label in y:\n",
        "            y_new.append(np.array(one_hot_dict[class_label]))\n",
        "        y_new = np.array(y_new)\n",
        "        y_new = pd.DataFrame(y_new)\n",
        "       \n",
        "        # Normalization\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X)\n",
        "        X_normal = scaler.transform(X)\n",
        "  \n",
        "        # Windowing\n",
        "        X_windowed,y_windowed = split_sequence_slide(X_normal,y_new,60)\n",
        "  \n",
        "        X_windowed = np.array(X_windowed)\n",
        "        y_windowed = np.array(y_windowed)\n",
        "        \n",
        "        model_input_x.append(X_windowed)\n",
        "        model_input_y.append(y_windowed)\n",
        "        \n",
        "        print(f\"sub{i} session{j}\")\n",
        "      \n",
        "        print(\"\\n\")\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "X=[]\n",
        "for arr in model_input_x:\n",
        "    for i in arr:\n",
        "        X.append(i)\n",
        "Y=[]\n",
        "for arr in model_input_y:\n",
        "    for i in arr:\n",
        "        Y.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHF-3BEv_Xan",
        "outputId": "4eda1bc2-924e-4541-b5da-010505b0da2b"
      },
      "outputs": [],
      "source": [
        "# Computing sample weights\n",
        "from sklearn.utils import compute_sample_weight   \n",
        "weights = compute_sample_weight(class_weight = 'balanced', y = Y)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XKcRngK7QGO",
        "outputId": "f3056816-8df1-4534-e69f-488d56d943b1"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "import numpy\n",
        "\n",
        "model_5 = Sequential()\n",
        "model_5.add(Bidirectional(LSTM(6, activation='relu', input_shape= (60,6),return_sequences=True)))\n",
        "model_5.add(Bidirectional(LSTM(12,dropout=0.5,recurrent_dropout=0.5,activation='relu')))\n",
        "model_5.add(Dense(24, activation='relu'))\n",
        "model_5.add(Dropout(.5))\n",
        "model_5.add(Dense(4, activation='softmax'))\n",
        "model_5.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "x_train = tf.cast(numpy.array(X) , dtype=tf.float32)\n",
        "y_train = tf.cast(numpy.array(Y) , dtype=tf.int32)\n",
        "model_5.fit(x_train,y_train,batch_size=64, epochs = 10, sample_weight=weights )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run only if needed\n",
        "model_5 = tf.keras.models.load_model('model_4.keras')\n",
        "print(model_5.summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5CLRV_ZL-ZB-",
        "outputId": "ceb3fc31-8150-4b1c-ae08-e69b44726e43"
      },
      "outputs": [],
      "source": [
        "#This cell handles output generation and storage\n",
        "\n",
        "dict_prediction ={9:1,10:1,11:1,12:1}\n",
        "\n",
        "for l in dict_prediction.keys():\n",
        "    for j in range(1,dict_prediction[l]+1):\n",
        "        \n",
        "        x_df = pd.read_csv(\"TestData/subject_0\"+str(l)+\"_0\"+str(j)+\"__x.csv\")\n",
        "        timestamp_length = len(pd.read_csv(\"TestData/subject_0\"+str(l)+\"_0\"+str(j)+\"__y_time.csv\"))\n",
        "        x_first = pd.DataFrame(x_df.columns).T\n",
        "        \n",
        "        x_columns = [\"a1\", \"a2\", \"a3\", \"g1\", \"g2\", \"g3\"]\n",
        "        x_df.columns = x_columns\n",
        "        x_first.columns = x_columns\n",
        "        x_df = pd.concat([x_first, x_df],axis = 0).reset_index(drop=True)\n",
        "\n",
        "        X = []\n",
        "        count = 0\n",
        "        #Normalization of input values\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(x_df)\n",
        "        X_normal = scaler.transform(x_df)\n",
        "\n",
        "        # Windowing\n",
        "        while(count <= len(x_df)-60):\n",
        "            arr_of_X_60 = np.array(X_normal[count: count + 60,:], dtype=np.float32)\n",
        "            count = count + 1\n",
        "            X.append(arr_of_X_60)\n",
        "\n",
        "        X = numpy.array(X)\n",
        "        # Prediction \n",
        "        model_output_y = model_5.predict(X)\n",
        "        # Assingning 1 to the class with the highest probabaility\n",
        "        model_output_y_new = tf.one_hot(tf.argmax(model_output_y,axis =1), depth = 4)\n",
        "        model_output_y_new = model_output_y_new.numpy()\n",
        "        model_output_y_new = model_output_y_new.astype(np.int64)\n",
        "\n",
        "        #Choosing one class for every 4 input sets because of the frequency of timestamps \n",
        "        pred_output =[]\n",
        "        for i in range(len(model_output_y_new)):\n",
        "            if i%4==0:\n",
        "                pred_output.append(list(model_output_y_new[i]).index(1))\n",
        "        #Assigning the last predicted value to the remaining classes if any\n",
        "        diff = timestamp_length - len(pred_output)\n",
        "        val = pred_output[-1]\n",
        "        for i in range(diff):\n",
        "            pred_output.append(val)\n",
        "\n",
        "\n",
        "        y_arr = numpy.array(pred_output)\n",
        "        y_arr = y_arr.transpose()\n",
        "        # Creating a csv file with the predicted values\n",
        "        import csv\n",
        "        with open(\"subject_0\"+str(l)+\"_0\"+str(j)+\"__y.csv\", mode = 'w',newline='') as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerows(map(lambda x:[x],y_arr))\n",
        "                \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
